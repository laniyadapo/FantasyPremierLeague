{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOWNLOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ingestion_content = \"\"\"\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "def data_ingestion():\n",
    "    ''''\n",
    "    Component to download data from a github repo.\n",
    "    '''\n",
    "    df_allseasons = pd.read_csv('https://raw.githubusercontent.com/vaastav/Fantasy-Premier-League/master/data/cleaned_merged_seasons.csv', index_col = 'Unnamed: 0')\n",
    "\n",
    "    # Get yearly historic data from endpoint for available seasons and identify the keys in each disctionary using 2016 as an example.\n",
    "\n",
    "    Y2016= requests.get('https://www.fantasynutmeg.com/api/history/season/2016-17').json()\n",
    "    Y2017= requests.get('https://www.fantasynutmeg.com/api/history/season/2017-18').json()\n",
    "    Y2018= requests.get('https://www.fantasynutmeg.com/api/history/season/2018-19').json()\n",
    "    Y2019= requests.get('https://www.fantasynutmeg.com/api/history/season/2019-20').json()  \n",
    "    Y2020= requests.get('https://www.fantasynutmeg.com/api/history/season/2020-21').json()\n",
    "    Y2021= requests.get('https://www.fantasynutmeg.com/api/history/season/2021-22').json()\n",
    "    Y2022= requests.get('https://www.fantasynutmeg.com/api/history/season/2022-23').json()\n",
    "\n",
    "    # Convert history data dictionary to a pandas dataframe.\n",
    "\n",
    "    hist16_df = pd.DataFrame(Y2016['history'])\n",
    "    hist17_df = pd.DataFrame(Y2017['history'])\n",
    "    hist18_df = pd.DataFrame(Y2018['history'])\n",
    "    hist19_df = pd.DataFrame(Y2019['history'])\n",
    "    hist20_df = pd.DataFrame(Y2020['history'])\n",
    "    hist21_df = pd.DataFrame(Y2021['history'])\n",
    "\n",
    "    # Engineer feature to highlight each season year.\n",
    "\n",
    "    hist16_df['year'] = hist16_df.apply(lambda x: \"2016-17\", axis=1)\n",
    "    hist17_df['year'] = hist17_df.apply(lambda x: \"2017-18\", axis=1)\n",
    "    hist18_df['year'] = hist18_df.apply(lambda x: \"2018-19\", axis=1)\n",
    "    hist19_df['year'] = hist19_df.apply(lambda x: \"2019-20\", axis=1)\n",
    "    hist20_df['year'] = hist20_df.apply(lambda x: \"2020-21\", axis=1)\n",
    "    hist21_df['year'] = hist21_df.apply(lambda x: \"2021-22\", axis=1)\n",
    "\n",
    "    # Concatenate all history data across years.\n",
    "\n",
    "    hist_df = [hist16_df, hist17_df, hist18_df, hist19_df, hist20_df, hist21_df]\n",
    "    hist = pd.concat(hist_df, axis = 0, ignore_index=True)\n",
    "\n",
    "\n",
    "    #get current season data from FPL API endpoints and identify the keys\n",
    "    fpl_base_url = 'https://fantasy.premierleague.com/api/'\n",
    "    current_season = requests.get(fpl_base_url+'bootstrap-static/').json()\n",
    "\n",
    "    #create dataframes for the current season dictionary keys for data exploration\n",
    "    #- Contains summary of Gameweek data\n",
    "    events_df = pd.DataFrame(current_season['events']) #\n",
    "    phases_df = pd.DataFrame(current_season['phases']) #Shows calendar months for game weeks\n",
    "    teams_df = pd.DataFrame(current_season['teams'])\n",
    "    players_df = pd.DataFrame(current_season['elements'])\n",
    "    element_stats_df = pd.DataFrame(current_season['element_stats'])\n",
    "    element_types_df = pd.DataFrame(current_season['element_types'])\n",
    "\n",
    "    for x in players_df.index :\n",
    "        player_id = players_df.id[x]\n",
    "        url = f'https://fantasy.premierleague.com/api/element-summary/{player_id}/'\n",
    "        r = requests.get(url)\n",
    "        json = r.json()\n",
    "        json_history_df = pd.DataFrame(json['history'])\n",
    "\n",
    "       \n",
    "        if x == 0 :\n",
    "            df_currentseason = json_history_df\n",
    "        else : \n",
    "            df_currentseason = df_currentseason.append(json_history_df)\n",
    "\n",
    "\n",
    "    #get current season fixtures from FPL API endpoint and create Dataframe\n",
    "    current_season_fixtures = requests.get(fpl_base_url+'fixtures/').json()\n",
    "    fixtures_df = pd.DataFrame(current_season_fixtures)\n",
    "\n",
    "    #Map the team names and the player positions into the players_df_clean dataframe\n",
    "    teams_now=dict(zip(teams_df.id, teams_df.short_name))\n",
    "    positions=dict(zip(element_types_df.id, element_types_df.singular_name_short))\n",
    "    players_df['club_name'] = players_df['team'].map(teams_now)\n",
    "    players_df['position'] = players_df['element_type'].map(positions)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    df_allseasons.to_csv('all_seasons_hist.csv')\n",
    "    hist.to_csv('hist_data.csv')\n",
    "    players_df.to_csv('players_df.csv')\n",
    "    df_currentseason.to_csv('currentseason.csv')\n",
    "    fixtures_df.to_csv('fixtures.csv')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_ingestion()\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_download_path = \"./data_download\"\n",
    "if not os.path.exists(data_download_path):\n",
    "    os.makedirs(data_download_path)\n",
    "\n",
    "with open(f\"{data_download_path}/data_download.py\", 'w') as data_download_file:\n",
    "    data_download_file.write(data_ingestion_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the content of the requirement.txt file\n",
    "download_req_content = \"\"\"\n",
    "pandas\n",
    "requests\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the content to the data preprocessing requirements.txt file\n",
    "\n",
    "download_req_path = data_download_path\n",
    "\n",
    "with open(f\"{download_req_path}/requirements.txt\", 'w') as download_req:\n",
    "    download_req.write(download_req_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the content of the data preprocessing Dockerfile\n",
    "\n",
    "data_dockerfile_content = f\"\"\"\n",
    "FROM python:3.10.4-slim-buster\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY {data_download_path}/requirements.txt /app\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "COPY {data_download_path}/data_download.py /app\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the contents to the data downloader Dockerfile\n",
    "\n",
    "data_dockerfile_path = data_download_path\n",
    "\n",
    "with open(f\"{data_dockerfile_path}/Dockerfile\", 'w') as data_dockerfile:\n",
    "    data_dockerfile.write(data_dockerfile_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A fuction to Build the docker image for a component\n",
    "\n",
    "def docker_image_build(image_name, image_tag):\n",
    "    # Build the docker image of the component\n",
    "    os.system(f\"docker build -t {image_name}:{image_tag} -f {dockerfile_dir}/Dockerfile .\")\n",
    "    # Tag the built docker image\n",
    "    os.system(f\"docker tag {image_name}:{image_tag} {username}/{image_name}:{image_tag}\")\n",
    "    # Push the image to the docker container registry\n",
    "    os.system(f\"docker push {username}/{image_name}:{image_tag}\")\n",
    "    docker_image = f\"{username}/{image_name}:{image_tag}\"\n",
    "    return docker_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  44.83MB\n",
      "Step 1/5 : FROM python:3.10.4-slim-buster\n",
      " ---> e00cda196d23\n",
      "Step 2/5 : WORKDIR /app\n",
      " ---> Using cache\n",
      " ---> 875dc2c67660\n",
      "Step 3/5 : COPY ./data_download/requirements.txt /app\n",
      " ---> Using cache\n",
      " ---> 2eaf33bec289\n",
      "Step 4/5 : RUN pip install -r requirements.txt\n",
      " ---> Using cache\n",
      " ---> 342f2dccdbf0\n",
      "Step 5/5 : COPY ./data_download/data_download.py /app\n",
      " ---> Using cache\n",
      " ---> 3ae22c1717a8\n",
      "Successfully built 3ae22c1717a8\n",
      "Successfully tagged data-download-fpl:latest\n",
      "The push refers to repository [docker.io/pelvic/data-download-fpl]\n",
      "85492addfff0: Preparing\n",
      "ab5918050c4c: Preparing\n",
      "bc3f4dc46e81: Preparing\n",
      "e189e6d19dae: Preparing\n",
      "c41bbba2c89c: Preparing\n",
      "361093c2629f: Preparing\n",
      "69b6043419ca: Preparing\n",
      "4bdae028fbe3: Preparing\n",
      "10e6bc6fdee2: Preparing\n",
      "361093c2629f: Waiting\n",
      "69b6043419ca: Waiting\n",
      "4bdae028fbe3: Waiting\n",
      "10e6bc6fdee2: Waiting\n",
      "bc3f4dc46e81: Layer already exists\n",
      "c41bbba2c89c: Layer already exists\n",
      "e189e6d19dae: Layer already exists\n",
      "85492addfff0: Layer already exists\n",
      "ab5918050c4c: Layer already exists\n",
      "361093c2629f: Layer already exists\n",
      "69b6043419ca: Layer already exists\n",
      "4bdae028fbe3: Layer already exists\n",
      "10e6bc6fdee2: Layer already exists\n",
      "latest: digest: sha256:a4c81b65b1c45cf81046eaefc9d6e8dbaa7fd018207cd8ec06b2dc1f9caa19ee size: 2203\n"
     ]
    }
   ],
   "source": [
    "# Defining the parameters for the docker image\n",
    "dockerfile_dir = data_dockerfile_path\n",
    "username = input('Enter your docker hub username: ')\n",
    "password = input('Enter your docker hub account password: ')\n",
    "image_name = 'data-download-fpl'\n",
    "image_tag = 'latest'\n",
    "\n",
    "# Logging into docker account\n",
    "os.system(f\"docker login -u {username} -p {password}\")\n",
    "\n",
    "# Building the data download image and pushing it to the container registry\n",
    "data_download_image = docker_image_build(image_name, image_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the content of the feature engineering script\n",
    "\n",
    "feature_eng_content = \"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "def feat_eng(args):\n",
    "    df_allseasons = pd.read_csv(args.allseasons)\n",
    "    hist = pd.read_csv(args.hist)\n",
    "    players_df = pd.read_csv(args.players_df)\n",
    "    df_currentseason = pd.read_csv(args.currentseason)\n",
    "    fixtures_df = pd.read_csv(args.fixtures_df)\n",
    "\n",
    "    # FOR THE TRAIN AND VALIDATION SET\n",
    "    # Engineer feature to highlight the form of the players.\n",
    "    hist['form'] = hist['total_points']/38 \n",
    "\n",
    "    # Engineer feature to highlight the players name and the season they played in.\n",
    "    hist['name_season'] = hist['first_name'] + ' ' + hist['second_name'] + '_' + hist['year']\n",
    "\n",
    "    # Engineer feature to highlight the players name and the season they played in.\n",
    "    df_allseasons['name_season'] = df_allseasons['name'] + '_' + df_allseasons['season_x']\n",
    "\n",
    "    # Engineer a feature to highlight the club of the player.\n",
    "    teams=dict(zip(hist.name_season, hist.team_name))\n",
    "\n",
    "    df_allseasons['club_name'] = df_allseasons['name_season'].map(teams)\n",
    "\n",
    "    # Engineer a feature to highlight the form of the player.\n",
    "    teams=dict(zip(hist.name_season, hist.form))\n",
    "\n",
    "    df_allseasons['form'] = df_allseasons['name_season'].map(teams)\n",
    "\n",
    "    # Engineer feature to highlight the game dates from kickoff_time.\n",
    "    df_allseasons['game_date'] = df_allseasons['kickoff_time'].str.replace('T', ' ')\n",
    "    df_allseasons['game_date'] = df_allseasons['game_date'].str.replace(':00Z', '')\n",
    "\n",
    "    # Convert game_date feature to appropriate dtype.\n",
    "    df_allseasons['game_date'] = pd.to_datetime(df_allseasons['game_date'])\n",
    "\n",
    "    # Engineer game season weather feature.\n",
    "    seasons = [1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 1]\n",
    "    month_to_season = dict(zip(range(1,13), seasons))\n",
    "    df_allseasons['game_weather'] = df_allseasons.game_date.dt.month.map(month_to_season)\n",
    "\n",
    "    # Engineer feature to highlights games that started before 13:00 (early starts) and those that started after 13:00 (late starts)\n",
    "    df_allseasons['start_label'] = np.where((df_allseasons['game_date'].dt.hour) < 13, 0, 1)\n",
    "    # Engineer feature to highlight the game year only.\n",
    "    df_allseasons['year'] = df_allseasons.game_date.dt.year\n",
    "\n",
    "    # Drop feature.\n",
    "    df_allseasons.drop('team_x', axis = 1, inplace=True)\n",
    "    # Drop all missing observations.\n",
    "    df_allseasons.dropna(inplace=True)\n",
    "\n",
    "    # Change dypes.\n",
    "    df_allseasons['team_h_score'] = df_allseasons['team_h_score'].astype(int)\n",
    "    df_allseasons['team_a_score'] = df_allseasons['team_a_score'].astype(int)\n",
    "\n",
    "    # Drop features.\n",
    "    df_allseasons.drop(['opponent_team', 'kickoff_time'], axis = 1, inplace=True)\n",
    "\n",
    "    # Drop features.\n",
    "    df_allseasons.drop(['season_x', 'name', 'name_season', 'fixture', 'game_date', 'round', 'element'], axis=1, inplace=True)\n",
    "    # Drop all players with zero playtime.\n",
    "    zero_minutes = df_allseasons[df_allseasons.minutes == 0].index\n",
    "    df_allseasons.drop(zero_minutes, axis = 0, inplace=True)\n",
    "    df_allseasons.set_index('year', inplace=True)\n",
    "\n",
    "    # FEATURING ENGINEERING FOR THE TEST SET\n",
    "    #create the player name feature\n",
    "    players_df['name'] = players_df['first_name'] + ' ' + players_df['second_name']\n",
    "    #Create season_x feature to align with the train data\n",
    "    df_currentseason['season_x'] = df_currentseason.apply(lambda x: \"2022-23\", axis=1)\n",
    "    #Map the team names, player names and form into the all current season data player dataframe\n",
    "    teams_map=dict(zip(players_df.id, players_df.name))\n",
    "    club_map=dict(zip(players_df.id, players_df.club_name))\n",
    "    opp_teams_map=dict(zip(players_df.team, players_df.club_name))\n",
    "    form_map=dict(zip(players_df.id, players_df.form))\n",
    "    position_map=dict(zip(players_df.id, players_df.position))\n",
    "    df_currentseason['name'] = df_currentseason['element'].map(teams_map)\n",
    "    df_currentseason['club_name'] = df_currentseason['element'].map(club_map)\n",
    "    df_currentseason['opp_team_name'] = df_currentseason['opponent_team'].map(opp_teams_map)\n",
    "    df_currentseason['form'] = df_currentseason['element'].map(form_map)\n",
    "    df_currentseason['position'] = df_currentseason['element'].map(form_map)\n",
    "\n",
    "    df_currentseason.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    play_zero_minutes = df_currentseason[df_currentseason.minutes == 0].index\n",
    "    df_currentseason.drop(play_zero_minutes, axis = 0, inplace=True)\n",
    "    df_currentseason.rename(columns= { 'round': 'GW' }, inplace=True)\n",
    "    df_currentseason['game_date'] = df_currentseason['kickoff_time'].str.replace('T', ' ')\n",
    "    df_currentseason['game_date'] = df_currentseason['game_date'].str.replace(':00Z', '')\n",
    "    df_currentseason['game_date'] = pd.to_datetime(df_currentseason['game_date'])\n",
    "    df_currentseason['game_weather'] = df_currentseason.game_date.dt.month.map(month_to_season) \n",
    "    df_currentseason['start_label'] = np.where((df_currentseason['game_date'].dt.hour) < 13, 0, 1)\n",
    "    # Engineer feature tp highlight the game year only.\n",
    "    df_currentseason['year'] = df_currentseason.game_date.dt.year\n",
    "    df_currentseason.drop(['game_date', 'season_x'], axis=1, inplace=True)\n",
    "    df_currentseason.drop(['opponent_team', 'fixture', 'kickoff_time'], axis=1, inplace=True)\n",
    "    df_currentseason.form = df_currentseason.form.astype(float)\n",
    "    df_currentseason.set_index('year', inplace=True)\n",
    "    df_currentseason.drop(['element', 'name'], axis = 1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    df_allseasons.to_csv('all_seasons_clean_hist.csv')\n",
    "    df_currentseason.to_csv('df_test.csv')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--allseasons')\n",
    "    parser.add_argument('--hist')\n",
    "    parser.add_argument('--currentseason')\n",
    "    parser.add_argument('--players_df')\n",
    "    parser.add_argument('--fixtures_df')\n",
    "    args = parser.parse_args()\n",
    "    feat_eng(args)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_eng_path = \"./feature_engineering\"\n",
    "if not os.path.exists(feature_eng_path):\n",
    "    os.makedirs(feature_eng_path)\n",
    "\n",
    "with open(f\"{feature_eng_path}/feat_eng.py\", 'w') as feature_eng_file:\n",
    "    feature_eng_file.write(feature_eng_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_req_content = \"\"\"\n",
    "pandas\n",
    "numpy\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the content to the feataure engineering requirements.txt file\n",
    "\n",
    "feature_req_path = feature_eng_path\n",
    "\n",
    "with open(f\"{feature_req_path}/requirements.txt\", 'w') as feat_req:\n",
    "    feat_req.write(feat_req_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the content of the data preprocessing Dockerfile\n",
    "\n",
    "feat_dockerfile_content = f\"\"\"\n",
    "FROM python:3.10.4-slim-buster\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY {feature_eng_path}/requirements.txt /app\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "COPY {feature_eng_path}/feat_eng.py /app\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the contents to the feature engineering Dockerfile\n",
    "\n",
    "feat_dockerfile_path = feature_eng_path\n",
    "\n",
    "with open(f\"{feat_dockerfile_path}/Dockerfile\", 'w') as feat_dockerfile:\n",
    "    feat_dockerfile.write(feat_dockerfile_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  44.83MB\n",
      "Step 1/5 : FROM python:3.10.4-slim-buster\n",
      " ---> e00cda196d23\n",
      "Step 2/5 : WORKDIR /app\n",
      " ---> Using cache\n",
      " ---> 875dc2c67660\n",
      "Step 3/5 : COPY ./feature_engineering/requirements.txt /app\n",
      " ---> Using cache\n",
      " ---> 888e1977dd2f\n",
      "Step 4/5 : RUN pip install -r requirements.txt\n",
      " ---> Using cache\n",
      " ---> 96428eaf41ee\n",
      "Step 5/5 : COPY ./feature_engineering/feat_eng.py /app\n",
      " ---> Using cache\n",
      " ---> 21726b347be5\n",
      "Successfully built 21726b347be5\n",
      "Successfully tagged feature_engineering_fpl:latest\n",
      "The push refers to repository [docker.io/pelvic/feature_engineering_fpl]\n",
      "98dccb570335: Preparing\n",
      "81ca3c379615: Preparing\n",
      "5f627872c29a: Preparing\n",
      "e189e6d19dae: Preparing\n",
      "c41bbba2c89c: Preparing\n",
      "361093c2629f: Preparing\n",
      "69b6043419ca: Preparing\n",
      "4bdae028fbe3: Preparing\n",
      "10e6bc6fdee2: Preparing\n",
      "361093c2629f: Waiting\n",
      "69b6043419ca: Waiting\n",
      "4bdae028fbe3: Waiting\n",
      "10e6bc6fdee2: Waiting\n",
      "c41bbba2c89c: Layer already exists\n",
      "e189e6d19dae: Layer already exists\n",
      "98dccb570335: Layer already exists\n",
      "5f627872c29a: Layer already exists\n",
      "81ca3c379615: Layer already exists\n",
      "361093c2629f: Layer already exists\n",
      "69b6043419ca: Layer already exists\n",
      "10e6bc6fdee2: Layer already exists\n",
      "4bdae028fbe3: Layer already exists\n",
      "latest: digest: sha256:a4676babb31f3bae186c17c98f0caa583c9fd7dc536cc7a26cc7cfb5ad401439 size: 2203\n"
     ]
    }
   ],
   "source": [
    "# Building the feature engineering image and pushing it to the container registry\n",
    "\n",
    "dockerfile_dir = feat_dockerfile_path\n",
    "\n",
    "image_name = 'feature_engineering_fpl'\n",
    "image_tag = 'latest'\n",
    "\n",
    "feature_eng_image = docker_image_build(image_name, image_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENCODING CATEGORICAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating the content of the encoding script\n",
    "\n",
    "encoding_content = \"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import argparse\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def data_encoding(args):\n",
    "    df_allseasons_final = pd.read_csv(args.train)\n",
    "    df_allseasons_final.set_index('year', inplace=True)\n",
    "    # Sort index (just in case).\n",
    "    df_allseasons_final.sort_index(inplace=True)\n",
    "    # Assign features and target variable.\n",
    "    features = df_allseasons_final.drop(['total_points'], axis = 1)\n",
    "    # target = df_allseasons_final['total_points']\n",
    "\n",
    "    # Convert dataframe to a dictionary.\n",
    "    features_dict = features.to_dict(orient='records')\n",
    "\n",
    "    dv_final = DictVectorizer(sparse=False) \n",
    "\n",
    "    # sparse = False makes the output is not a sparse matrix.\n",
    "\n",
    "    features_encoded = dv_final.fit_transform(features_dict)\n",
    "    vocab_final = dv_final.vocabulary_\n",
    "    features_transformed = pd.DataFrame(features_encoded, columns=dv_final.feature_names_)\n",
    "    # Normalizing the train data.\n",
    "    min_max_scaler_final = MinMaxScaler()\n",
    "\n",
    "    # Fit scalar and transform train data.\n",
    "    features_norm = min_max_scaler_final.fit_transform(features_transformed)\n",
    "\n",
    "    data_path = './data_encoding/preprocessed_data'\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "    np.save(f'{data_path}/features.npy', features_norm)\n",
    "\n",
    "    if not os.path.exists('./model'):\n",
    "        os.makedirs('./model')\n",
    "\n",
    "    with open('./model/dv', 'wb') as f_out2:\n",
    "        pickle.dump(dv_final, f_out2)\n",
    "\n",
    "    with open('./model/min_max_scaler', 'wb') as f_out3:\n",
    "        pickle.dump(min_max_scaler_final, f_out3)\n",
    "\n",
    "    df_allseasons_final.to_csv('target.csv')\n",
    "\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train')\n",
    "    args = parser.parse_args()\n",
    "    data_encoding(args) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the contents to the data_encoding.py\n",
    "\n",
    "data_encoding_path = \"./data_encoding\"\n",
    "if not os.path.exists(data_encoding_path):\n",
    "    os.makedirs(data_encoding_path)\n",
    "\n",
    "with open(f\"{data_encoding_path}/data_encoding.py\", 'w') as data_encode_file:\n",
    "    data_encode_file.write(encoding_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the content of the requirement.txt file\n",
    "\n",
    "encoding_req_content = \"\"\"\n",
    "sklearn\n",
    "pandas\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the content to the data encoding requirements.txt file\n",
    "\n",
    "encoding_req_path = data_encoding_path\n",
    "\n",
    "with open(f\"{encoding_req_path}/requirements.txt\", 'w') as encoding_req:\n",
    "    encoding_req.write(encoding_req_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the content of the data encoding Dockerfile\n",
    "\n",
    "encoding_dockerfile_content = f\"\"\"\n",
    "FROM python:3.10.4-slim-buster\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY {data_encoding_path}/requirements.txt /app\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "COPY {data_encoding_path}/data_encoding.py /app\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the contents to the model training Dockerfile\n",
    "\n",
    "encoding_dockerfile_path = data_encoding_path\n",
    "\n",
    "with open(f\"{encoding_dockerfile_path}/Dockerfile\", 'w') as encoding_dockerfile:\n",
    "    encoding_dockerfile.write(encoding_dockerfile_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  44.84MB\n",
      "Step 1/5 : FROM python:3.10.4-slim-buster\n",
      " ---> e00cda196d23\n",
      "Step 2/5 : WORKDIR /app\n",
      " ---> Using cache\n",
      " ---> 875dc2c67660\n",
      "Step 3/5 : COPY ./data_encoding/requirements.txt /app\n",
      " ---> Using cache\n",
      " ---> 29a16b63819d\n",
      "Step 4/5 : RUN pip install -r requirements.txt\n",
      " ---> Using cache\n",
      " ---> 3921c117b536\n",
      "Step 5/5 : COPY ./data_encoding/data_encoding.py /app\n",
      " ---> 6ae6f56103cf\n",
      "Successfully built 6ae6f56103cf\n",
      "Successfully tagged data-encoding-fpl:latest\n",
      "The push refers to repository [docker.io/pelvic/data-encoding-fpl]\n",
      "153ca746b344: Preparing\n",
      "e1a01a7715d4: Preparing\n",
      "07c546a44f44: Preparing\n",
      "e189e6d19dae: Preparing\n",
      "c41bbba2c89c: Preparing\n",
      "361093c2629f: Preparing\n",
      "69b6043419ca: Preparing\n",
      "4bdae028fbe3: Preparing\n",
      "361093c2629f: Waiting\n",
      "69b6043419ca: Waiting\n",
      "10e6bc6fdee2: Preparing\n",
      "4bdae028fbe3: Waiting\n",
      "10e6bc6fdee2: Waiting\n",
      "c41bbba2c89c: Layer already exists\n",
      "e189e6d19dae: Layer already exists\n",
      "07c546a44f44: Layer already exists\n",
      "e1a01a7715d4: Layer already exists\n",
      "361093c2629f: Layer already exists\n",
      "69b6043419ca: Layer already exists\n",
      "4bdae028fbe3: Layer already exists\n",
      "10e6bc6fdee2: Layer already exists\n",
      "153ca746b344: Pushed\n",
      "latest: digest: sha256:a0b50b7f88a223a521c1dc498fc51e5689e3dd4636ed3b095bab773ac1780dd8 size: 2203\n"
     ]
    }
   ],
   "source": [
    "# Building the model training image and pushing it to the container registry\n",
    "\n",
    "dockerfile_dir = encoding_dockerfile_path\n",
    "\n",
    "image_name = 'data-encoding-fpl'\n",
    "image_tag = 'latest'\n",
    "\n",
    "data_encoding_image = docker_image_build(image_name, image_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL TRAINING AND TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating the content of the model training script\n",
    "\n",
    "model_train_content = \"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def model_training(args):\n",
    "    df_test = pd.read_csv(args.test)\n",
    "    df_test.set_index('year', inplace=True)\n",
    "    print(df_test.head())\n",
    "    df_allseasons_final = pd.read_csv(args.target)\n",
    "    print(df_allseasons_final.head())\n",
    "    target = df_allseasons_final['total_points']\n",
    "    features_norm = np.load(args.features_path, allow_pickle=True)\n",
    "    df_test_dict = df_test.to_dict(orient='records')\n",
    "    rf = RandomForestRegressor(random_state=2)\n",
    "    final_model = rf.fit(features_norm, target)\n",
    "\n",
    "    with open(args.dv_path, 'rb') as f_in1:\n",
    "        dv = pickle.load(f_in1)\n",
    "    with open(args.scaler_path, 'rb') as f_in2:\n",
    "        scaler = pickle.load(f_in2)\n",
    "\n",
    "    test_encoded = dv.transform(df_test_dict)\n",
    "    vocab = dv.vocabulary_\n",
    "    test_transformed = pd.DataFrame(test_encoded, columns=dv.feature_names_)\n",
    "    test_norm = scaler.transform(test_transformed)\n",
    "    print(test_norm)\n",
    "    predicted = final_model.predict(test_norm)\n",
    "    print(predicted[0:11])\n",
    "    # df_predicted = pd.Series(predicted)\n",
    "    RSME_score = mean_squared_error(y_true=df_test['total_points'], y_pred=predicted, squared=False) #squared=False will RMSE instead of MSE\n",
    "    R2_score = r2_score(df_test['total_points'], predicted)\n",
    "\n",
    "    print('RMSE:', RSME_score)\n",
    "    print('R-Squared:', R2_score)\n",
    "    print()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--features_path')\n",
    "    parser.add_argument('--target')\n",
    "    parser.add_argument('--test')\n",
    "    parser.add_argument('--dv_path')\n",
    "    parser.add_argument('--scaler_path')\n",
    "    args = parser.parse_args()\n",
    "    model_training(args) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the contents to the model_train.py\n",
    "\n",
    "model_train_path = \"./model_training\"\n",
    "if not os.path.exists(model_train_path):\n",
    "    os.makedirs(model_train_path)\n",
    "\n",
    "with open(f\"{model_train_path}/model_train.py\", 'w') as model_train_file:\n",
    "    model_train_file.write(model_train_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the content of the requirement.txt file\n",
    "\n",
    "train_req_content = \"\"\"\n",
    "pandas\n",
    "sklearn\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the content to the model training requirements.txt file\n",
    "\n",
    "train_req_path = model_train_path\n",
    "\n",
    "with open(f\"{train_req_path}/requirements.txt\", 'w') as train_req:\n",
    "    train_req.write(train_req_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the content of the model training Dockerfile\n",
    "\n",
    "train_dockerfile_content = f\"\"\"\n",
    "FROM python:3.10.4-slim-buster\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY {model_train_path}/requirements.txt /app\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "COPY {model_train_path}/model_train.py /app\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the contents to the model training Dockerfile\n",
    "\n",
    "train_dockerfile_path = model_train_path\n",
    "\n",
    "with open(f\"{train_dockerfile_path}/Dockerfile\", 'w') as train_dockerfile:\n",
    "    train_dockerfile.write(train_dockerfile_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  44.84MB\n",
      "Step 1/5 : FROM python:3.10.4-slim-buster\n",
      " ---> e00cda196d23\n",
      "Step 2/5 : WORKDIR /app\n",
      " ---> Using cache\n",
      " ---> 875dc2c67660\n",
      "Step 3/5 : COPY ./model_training/requirements.txt /app\n",
      " ---> Using cache\n",
      " ---> 1294e7db94df\n",
      "Step 4/5 : RUN pip install -r requirements.txt\n",
      " ---> Using cache\n",
      " ---> 2264788c19c6\n",
      "Step 5/5 : COPY ./model_training/model_train.py /app\n",
      " ---> 95aef0398f53\n",
      "Successfully built 95aef0398f53\n",
      "Successfully tagged model_train-fpl:latest\n",
      "The push refers to repository [docker.io/pelvic/model_train-fpl]\n",
      "62c2965047f4: Preparing\n",
      "409ad426b17f: Preparing\n",
      "4663a8c45f5b: Preparing\n",
      "e189e6d19dae: Preparing\n",
      "c41bbba2c89c: Preparing\n",
      "361093c2629f: Preparing\n",
      "69b6043419ca: Preparing\n",
      "4bdae028fbe3: Preparing\n",
      "10e6bc6fdee2: Preparing\n",
      "69b6043419ca: Waiting\n",
      "4bdae028fbe3: Waiting\n",
      "10e6bc6fdee2: Waiting\n",
      "361093c2629f: Waiting\n",
      "4663a8c45f5b: Layer already exists\n",
      "c41bbba2c89c: Layer already exists\n",
      "e189e6d19dae: Layer already exists\n",
      "409ad426b17f: Layer already exists\n",
      "361093c2629f: Layer already exists\n",
      "69b6043419ca: Layer already exists\n",
      "4bdae028fbe3: Layer already exists\n",
      "10e6bc6fdee2: Layer already exists\n",
      "62c2965047f4: Pushed\n",
      "latest: digest: sha256:ad6d149e3583b5db48fd7457b54d90080716abba72a5909d66c4dc8a538b4f69 size: 2203\n"
     ]
    }
   ],
   "source": [
    "# Building the model training image and pushing it to the container registry\n",
    "\n",
    "dockerfile_dir = train_dockerfile_path\n",
    "\n",
    "image_name = 'model_train-fpl'\n",
    "image_tag = 'latest'\n",
    "\n",
    "model_train_image = docker_image_build(image_name, image_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (1.8.13)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from kfp) (0.8.10)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.14 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from kfp) (0.1.16)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from kfp) (0.14.1)\n",
      "Requirement already satisfied: google-cloud-storage<2,>=1.20.0 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from kfp) (1.44.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from kfp) (2.8.2)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from kfp) (1.12.11)\n",
      "Requirement already satisfied: jsonschema<4,>=3.0.1 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from kfp) (3.2.0)\n",
      "Requirement already satisfied: Deprecated<2,>=1.2.7 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from kfp) (1.2.13)\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from kfp) (3.20.1)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from kfp) (1.9.2)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from kfp) (2.1.0)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from kfp) (0.9.1)\n",
      "Requirement already satisfied: fire<1,>=0.3.1 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from kfp) (0.4.0)\n",
      "Requirement already satisfied: absl-py<2,>=0.9 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from kfp) (1.2.0)\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from kfp) (5.4.1)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from kfp) (8.1.3)\n",
      "Requirement already satisfied: uritemplate<4,>=3.0.1 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from kfp) (3.0.1)\n",
      "Requirement already satisfied: strip-hints<1,>=0.1.8 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from kfp) (0.1.10)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from kfp) (1.8.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.1 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from kfp) (1.35.0)\n",
      "Requirement already satisfied: kubernetes<19,>=8.0.0 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from kfp) (18.20.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.3.2 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from kfp) (0.6.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from Deprecated<2,>=1.2.7->kfp) (1.14.1)\n",
      "Requirement already satisfied: termcolor in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from fire<1,>=0.3.1->kfp) (1.1.0)\n",
      "Requirement already satisfied: six in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from fire<1,>=0.3.1->kfp) (1.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.56.4)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2.28.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from google-api-python-client<2,>=1.7.8->kfp) (0.1.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from google-api-python-client<2,>=1.7.8->kfp) (0.20.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from google-auth<2,>=1.6.1->kfp) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from google-auth<2,>=1.6.1->kfp) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from google-auth<2,>=1.6.1->kfp) (4.9)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from google-auth<2,>=1.6.1->kfp) (61.2.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from google-cloud-storage<2,>=1.20.0->kfp) (2.3.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from google-cloud-storage<2,>=1.20.0->kfp) (2.3.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from jsonschema<4,>=3.0.1->kfp) (22.1.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from jsonschema<4,>=3.0.1->kfp) (0.18.1)\n",
      "Requirement already satisfied: urllib3>=1.15 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (1.26.11)\n",
      "Requirement already satisfied: python-dateutil in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2.8.2)\n",
      "Requirement already satisfied: certifi in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2022.6.15)\n",
      "Requirement already satisfied: requests-oauthlib in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from kubernetes<19,>=8.0.0->kfp) (1.3.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from kubernetes<19,>=8.0.0->kfp) (1.3.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from pydantic<2,>=1.8.2->kfp) (4.3.0)\n",
      "Requirement already satisfied: wheel in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from strip-hints<1,>=0.1.8->kfp) (0.37.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client<2,>=1.7.8->kfp) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/user/miniconda3/envs/myenv/lib/python3.9/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp) (3.2.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('pip install kfp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the components together\n",
    "\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "\n",
    "def data_download_op():\n",
    "    return dsl.ContainerOp(\n",
    "        name='Download Data',\n",
    "        image=f'{data_download_image}',\n",
    "        arguments=[],\n",
    "        command=[\"python\", \"data_download.py\"],\n",
    "        file_outputs={\n",
    "            'allseasons': '/app/all_seasons_hist.csv',\n",
    "            'hist': '/app/hist_data.csv',\n",
    "            'players': '/app/players_df.csv',\n",
    "            'currentseason': '/app/currentseason.csv',\n",
    "            'fixtures': '/app/fixtures.csv',\n",
    "        }\n",
    "    )\n",
    "\n",
    "def feature_eng_op(allseasons, hist,  players_df, currentseason, fixtures_df):\n",
    "    return dsl.ContainerOp(\n",
    "        name='Feature Engineering',\n",
    "        image=f'{feature_eng_image}',\n",
    "        arguments=[\n",
    "            '--allseasons', allseasons,\n",
    "            '--hist', hist,\n",
    "            '--players_df', players_df,\n",
    "            '--currentseason', currentseason,\n",
    "            '--fixtures_df', fixtures_df\n",
    "        ],\n",
    "        command=[\"python\", \"feat_eng.py\"],\n",
    "        file_outputs={\n",
    "            'train': '/app/all_seasons_clean_hist.csv',\n",
    "            'test': '/app/df_test.csv',\n",
    "        }\n",
    "    )\n",
    "\n",
    "def data_encoding_op(train):\n",
    "    return dsl.ContainerOp(\n",
    "        name='Data Encoding',\n",
    "        image=f'{data_encoding_image}',\n",
    "        arguments=[\n",
    "            '--train', train\n",
    "        ],\n",
    "        command=[\"python\", \"data_encoding.py\"],\n",
    "        file_outputs={\n",
    "            'feature_path': '/app/data_encoding/preprocessed_data/features.npy',\n",
    "            'target': '/app/target.csv',\n",
    "            'dv':'/app/model/dv',\n",
    "            'scaler' : '/app/model/min_max_scaler',\n",
    "        }\n",
    "    )\n",
    "\n",
    "def model_train_op(features_path, target, dv_path, scaler_path, test):\n",
    "    return dsl.ContainerOp(\n",
    "        name='Train Model',\n",
    "        image=f'{model_train_image}',\n",
    "        arguments=[\n",
    "            '--features_path', features_path,\n",
    "            '--target',target,\n",
    "            '--dv_path', dv_path,\n",
    "            '--scaler_path', scaler_path,\n",
    "            '--test', test\n",
    "        ],\n",
    "        command=[\"python\", \"model_train.py\"],\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "   name='REGRESSION ML workflow pipeline',\n",
    "   description='A pipeline for a regressor job for predicting the total points in FPL'\n",
    ")\n",
    "def FPL_pipeline():\n",
    "    _data_download_op = data_download_op()\n",
    "\n",
    "    _feature_eng_op = feature_eng_op(\n",
    "        dsl.InputArgumentPath(_data_download_op.outputs['allseasons']),\n",
    "        dsl.InputArgumentPath(_data_download_op.outputs['hist']),\n",
    "        dsl.InputArgumentPath(_data_download_op.outputs['players']),\n",
    "        dsl.InputArgumentPath(_data_download_op.outputs['currentseason']),\n",
    "        dsl.InputArgumentPath(_data_download_op.outputs['fixtures']),\n",
    "    ).after(_data_download_op)\n",
    "\n",
    "    _data_encoding_op = data_encoding_op(\n",
    "        dsl.InputArgumentPath(_feature_eng_op.outputs['train']),\n",
    "    ).after(_feature_eng_op)\n",
    "   \n",
    "    _model_train_op = model_train_op(\n",
    "        dsl.InputArgumentPath(_data_encoding_op.outputs['feature_path']),\n",
    "        dsl.InputArgumentPath(_data_encoding_op.outputs['target']),\n",
    "        dsl.InputArgumentPath(_data_encoding_op.outputs['dv']),\n",
    "        dsl.InputArgumentPath(_data_encoding_op.outputs['scaler']),\n",
    "        dsl.InputArgumentPath(_feature_eng_op.outputs['test']),\n",
    "    ).after(_data_encoding_op)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    kfp.compiler.Compiler().compile(FPL_pipeline, 'FPL_pipeline.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "53709a8ccd3fd9ba97e4a13313240f4dec31d4bbc259d1945af48ebf95b2d870"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
